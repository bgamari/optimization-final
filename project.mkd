% Maximum entropy techniques for fluorescence correlation spectroscopy
% Ben Gamari
% Optimization final project

# Motivation

Fluorescence correlation spectroscopy[@Magde1972] (or more correctly, fluorescence
fluctuation correlation spectroscopy) is a analytic technique used for
characterizing the static and dynamic behavior of molecular and
macromolecular species. In an FCS experiment, the specimen of interest
is labelled with a fluorescent molecule (a dye) and allowed to diffuse in
solution. A small observation volume of this solution is illuminated
by a confocal microscope. As molecules pass through this volume, their dyes are
excited and their emissions are collected as time-tagged photon
observations. This produces a fluorescence intensity timeseries
$I(t)$. This timeseries can be decomposed into a steady state
component $\bar I = \langle I \rangle$ and time dependent
fluctuations, $\delta I(t)$,

$$ I(t) = \bar I + \delta I(t). $$

![Typical experimental setup for FCS experiment](fret-setup.pdf)

We begin by examining the temporal autocorrelation of the fluctuation
term, that is, the correlation between the intensity at time $t$ and a
time $\tau$ in the future, $t+\tau$. This is given by, (up to
normalization),

\begin{equation}
G(\tau) = \int dt~ \delta I(t)~ \delta I(t+\tau),
\end{equation}

one can draw conclusions about the characteristics of the
system under study.

In the most simple case, we are interested in the diffusivity $D$,
which summarizes the diffusive characteristics of a normally diffusing
particle. Intuitively, $D$ captures the expected displacement in a
given time.

Consider a diffusing particle whose position is known to be
$\mathbf{r}_0$ at time $t=0$. Equivalently, we say that the
probability over the particle's position is given by $P(\mathbf{r}
\vert t=0) = \delta(\mathbf{r}_0 - \mathbf{r})$. The Dirac
$\delta$ function can be taken to be a limit of Gaussians with
vanishing variance as a function of time,

\begin{equation}
P(\mathbf{r} ~\vert~ t=0) = \lim_{\sigma \rightarrow \infty} \mathcal{N}(\mathbf{r}_0, \sigma^2)
\end{equation}

where $\mathcal{N}$ denotes the PDF of the normal distribution. 
In light of this, we can view the diffusivity $D$ to be the rate of
increase in the variance of our particle's position,

\begin{equation}
P(\mathbf{r} ~\vert~ t, \mathbf{r}(t=0) = \mathbf{r}_0) = \mathcal{N}(\mathbf{r}_0, D\, t)
\end{equation}

N.B. We have made a few implicit assumptions above; namely those characterizing 
"normal diffusion". To show that this assumption of normality is valid, one would need to solve the diffusion equation with the relevant boundary conditions, which is beyond the scope of this paper. Nevertheless, in many systems of interest, in particular those
exhibiting diffusion through sparsely connected spaces such as the
interstitial spaces of a polymer, a network, or a porous material like
sand, one finds anomalous diffusion, where the $\sigma^2$ is no longer
a linear function of time but instead a power law.

![A correlation function showing roughly single component normal diffusion. Note the systematically large residuals, likely due to unaccounted for photophysics. As the timescales of the dynamics seen in a correlation experiment span several orders of magnitude, it is typical to plot the function on a logarithmic abscissa.](2012-12-20-run_005-acorr-0.pdf)

The analysis of a typical FCS experiment begins with the calculation
of the correlation function which can be carried out by a number of
methods[@Laurence2006, @Gamari2013, @Magatti2001].  This function can
then be fit against any one of a variety of models, depending upon the
dynamics expected in the data. In the most common case, one is looking
exclusively for normal three-dimensional diffusion, characterized by
correlations of the form,

\begin{equation}
G^*(\tau) = \frac{1}{N} \left(\frac{1}{1 + \tau / \tau_D}\right) \left(\frac{1}{1 + (r/l)^2 (\tau/\tau_D)}\right) \label{Eq:3dDiff}
\end{equation}

where $\tau_D$ is the characteristic diffusion time of the species,
$r$ and $l$ are geometric parameters of the experimental apparatus,
and $N$ is the expected number of labelled molecules in the
observation (e.g. the concentration of the species in solution).

The parameters $\tau_D$, $N$, $r$, and $l$ are typically learned by
way of least-squares fit. Here we minimize $\chi^2$, a weighted average of the
squared residual,

$$ \chi^2 = \frac{1}{n} \sum_{i=1}^n \frac{(G^*(\tau_i) - G(\tau_i))^2}{\sigma_i^2} $$

Where $\sigma$ is the standard error of the $G$ estimator. This quantity
can be estimated under a number of different modelling
assumptions[@Wohland1986, @Schatzel1990, @Schatzel1991, @Gamari2013, @Culbertson2007, @Sarrarian2003]. The
most common treatment of the error is due to Koppel[@Koppel1974] and
appeals to an assumption of temporal independence (an admittedly
suspicous assumption for an experiment looking at temporal
correlations).

For reasons of convenience and ubiquity, the
Levenburg-Marquadt[@Levenburg1944, @Marquadt1963]
algorithm is typically used for performing this optimization.  While
in many common cases the objective is convex, it is not difficult to
find heterogeneous systems which give rise to more challenging
problems. Recently our lab has been working to characterize a
colloidal water/oil system. To better understand droplet size
distribution, we use FCS to measure the diffusivity distribution
of the colloid which is heterogeneous\footnote{and sadly dynamic, but we will
brush this under the rug for now}.

![A representative landscape of $\chi^2$ in the $\tau_D - \tau_F$ plane](simple-landscape.pdf)

To write down the expected correlation found in such a system, we
first consider the correlations $G_1$ and $G_2$ . The autocorrelation of a system
consisting these two species will be,

$$ G(\tau) = \int dt (\delta I_1(t) + \delta I_2(t)) (\delta I_1(t+\tau) + \delta I_2(t+\tau)) $$

Expanding the product, we find four terms,

$$ G(\tau) = \int dt \left[ \delta I_1(t)\: \delta I_1(t+\tau) + \delta I_1(t)\: \delta I_2(t+\tau) + \delta I_2(t)\: \delta I_1(t+\tau) + \delta I_2(t)\: \delta I_2(t+\tau) \right] $$

We note that first note that the motions of the particles of the species 1
are independent of those of species 2, therefore the two cross terms
vanish. We are left with the weighted sum of the two single-species
autocorrelations,

$$ G(\tau) = G_1(\tau) + G_2(\tau), $$

Or in the case of the diffusion model given in Eq. \ref{Eq:3dDiff},

$$ G(\tau) = \sum_i a_i \left(\frac{1}{1 + \tau / \tau_{D_i}}\right) \left(\frac{1}{1 + (r/l)^2 (\tau/\tau_{D_i})}\right) $$

Where $a_i$ are the relative abundances of the two species. In the
common case of a continuuous spectrum of diffusivities we can write
this sum as an integral, which for computational ease we can rewrite as a
sum of appropriately many discrete contributions.

# Maximum entropy approaches to modelling

Such heterogeneous distributions present great difficulty for the
optimization approach given above. Especially in the presence of
finite data, a naive least squares fit will generally either fit only
to the dominant species or fail to converge at all.

In 2003, Sengupta *et. al.* brought Skilling and Bryan's work on 
image reconstruction to bear on this problem. Here, Skilling
identifies the entropy of an image of intensities $f_i$,

$$ S = -\sum_i P(f_i) \log P(f_i), $$

as a unique criterion for selecting among models. Skilling appeals to
the earlier work of Shore, positing that a maximum entropy model will
avoid adding any correlations beyond those necessary for consistency
with observations.

Sengupta reformulated the FCS fitting problem as a constrained
optimization, minimizing the entropy of the species' weights
under a constraint on $\chi^2$. This attempts to maximize the
non-informativeness of the inference while ensuring agreement with
observation.

## Statement of problem

We are given an observed correlation function $G(\tau)$. Given a set
of diffusion times $\{\tau_{D_i}\}$ which we might expect to see in
the system, we want to learn a set of component weights $\mathbf{a}
\ge 0$ and auxiliary model parameters. We demand that our weight
distribution maximizes entropy $S$ while satifying an inequality
constraint on $\chi^2$,

\begin{align}
& \max_{\mathbf{a}} \left[ -\sum_i a_i \log a_i \right] \\
\mathrm{such~ that~} 
& a_i \ge 0 \\
& \sum_i a_i = 1 \\
& \sum_j \frac{(G(\tau_j) - G^*(\tau_j ~\vert~ \mathbf{a}))^2}{\sigma_j^2} \le c \label{Eq:ChiConstr}
\end{align}

for some value $c$ specifying the degree of misfit we are willing to accept.
We note that the weights $a$ are simply a prefactor in $G^*$. Consequently we can define 
the quantities,

\begin{align*}
\hat G^*_{ij} & = \left(\frac{1}{1 + \tau_j / \tau_{D_i}}\right) \left(\frac{1}{1 + (r/l)^2 (\tau_j/\tau_{D_i})}\right) \left(\frac{1}{\sigma_j^2}\right) \\
\hat G_j      & = \frac{G(\tau_j)}{\sigma_j^2},
\end{align*}

which allows us to write our constraint Eq. \ref{Eq:ChiConstr} more concisely as,

\begin{equation}
\sum_j \left( \hat{G}_j - a^i \hat{G}^*_{ij} \right)^2 \le c. \label{Eq:ChiConstr2}
\end{equation}

## Approaches

Here we will examine several approaches for solving the above optimization problem,

  * Skilling and Bryan's original conjugate gradient algorithm as used by Sengupta
  * projected subgradient method
  * mirror descent method
  * logarithmic barrier method

### Skilling's conjugate gradient

Skilling and Bryan's algorithm is a standard conjugate gradient
technique with some tricks for treating the constraint on $\chi$. The
interested reader is referred to the paper[@Skilling1984] for further
details. Sengupta makes minimal changes to this original technique.

### Projected subgradient method

To employ the projected subgradient method, we will first need a
projection operator. Expanding the constraint from
Eq. \ref{Eq:ChiConstr2},

\begin{equation*}
\sum_j \left( - 2 \hat{G}_j a^i \hat{G}^*_{ij} + (a^i \hat{G}^*_{ij})^2 \right) \le c - \sum_j \hat{G}_j^2.
\end{equation*}

After further juggling of terms we find,

\begin{equation}
a^i A_i + a^i a_k B_i^k \le c - \sum_j \hat{G}_j^2. \label{Eq:ChiConstr3}
\end{equation}
where,
\begin{align*}
A_i   & = \sum_j \hat{G}_j \hat{G}^*_{ij} \\
B_i^k & = \sum_j (\hat{G}^*)_{ij} (\hat{G}^*)^{jk}.
\end{align*}

As expected, this is manifestly a quadratic constraint. We desire a
projection operator $\mathcal{P}(\mathbf{a})$ which brings $\mathbf a$
to a point on the quadric surface defined by $\mathbf{a}^T B
\mathbf{a} + \mathbf{a} A = c$.  One reasonable choice of projections
would be to identify the point $\mathbf{a'}$ on this surface that
minimizes $\left\Vert \mathbf{a} - \mathbf{a'} \right\Vert $. While
this could be accomplished by the method of Lagrange multipliers, we'd
like to avoid excess complexity in our projection. Instead, we prefer
to use a direct line search in the direction of the gradient of our
constraint. That is, given a step factor $\alpha$, we start with,

\begin{equation}
\mathbf{p}_0 = \nabla_\mathbf{a} \mathbf{a}^T B \mathbf{a} + A \mathbf{a},
\end{equation}

and perform the iteration,

\begin{equation}
\mathbf{p}_n = \alpha \mathbf{p}_{n-1}
\end{equation}

until Eq. \ref{Eq:ChiConstr3} is satisfied by $\mathbf{a}' =
\mathbf{a} + \mathbf{p}_n$. Note that, for simplicity, this ensures
only that the constraint is satisfied and makes no attempt to ensure
optimality. Namely, our projection is not ensured to be on the
boundary of our space, where the optimal projection is known to
lie. Before and after this projection we will scale $\mathbf{a}$ to
ensure that the normalization constraint is met.

With this projection operator in hand, we can carry out the typical
projected subgradient iteration,

\begin{equation*}
\mathbf{a}_{n+1} = \mathcal{P}\left( \mathbf{a}_n + t_n \nabla_\mathbf{a} \left[ \sum_i (a_n)_i \log (a_n)_i \right] \right).
\end{equation*}

### Mirror descent method

Following Beck and Taboulle[@Beck2003], we use the mirror descent
algorithm choosing the auxilary function $\psi(\mathbf{a})$ to be the
entropy function. However, in addition to the constraint to the unit
simplex, we also need to enforce our constraint on $\chi^2$. For this
reason, we define $\psi$ as follows,

\begin{equation}
\psi(\mathbf{a}) = \sum_j \mathbf{a}_j \log \mathbf{a}_j + I_\Delta(\mathbf{a}) + I_C(\mathbf{a})
\end{equation}

where $\Delta = \{ x \in \mathbb{R}^n : \sum_j x_j = 1, x \ge 0\}$
denotes the unit simplex, $C$ denotes the set of solutions satisfying
Eq. \ref{Eq:ChiConstr3}, and the indicator function $I_A$ for a set
$A$ is defined as,

\begin{equation}
I_A(\mathbf{x}) = \left\{
\begin{array}{lr}
+\infty    &  x \in C \\
0          & \mathrm{otherwise} 
\end{array}
\right.
\end{equation}


### Logarithmic barrier method

Barrier methods are a class of interior point methods which employ a
penalty term in the objective to discourage violation of the program's
constraints. As the weight of the penalty is increased, the algorithm
converges to an optimal, feasible solution.

We consider the case of a logarithmic barrier function, where the
modified objective is given by,

\begin{equation}
f(\mathbf{a}) = \sum_i \mathbf{a}_i \log \mathbf{a}_i + \mu \left( B_{\chi^2} + B_\mathrm{norm} + B_\mathrm{nonneg} \right)
\end{equation}

Where the barrier function for the $\chi^2$ and non-negativity constraints are given by,
\begin{align*}
B_{\chi^2} & = \log \left[ \mathbf{a}^T B \mathbf{a} + A \mathbf{a} - c + \sum_j \hat{G}_j^2 \right] \\
B_\mathrm{nonneg} & = \sum_i \log \mathbf{a}_i
\end{align*}

To treat the normalization (equality) constraint, we could use a quotient to
punish deviations in the one-norm away from one,

\begin{equation*}
B_\mathrm{norm} = \log \frac{1}{\| \mathbf{a} \|_1^2}
\end{equation*}

Unfortunately, this constraint will be substantially softer than the others.
Another (perhaps less principled) approach for the normalization
constraint is to project each iterate into the space of normalized
solutions by simply renormalizing. This is the approach used below.

While these barriers may still differ by orders of magnitude, we tie
them together with the same $\mu$ for simplicity.

A trust region method such as Newton's method is often a good choice
for carrying out this optimization due to strong convergence
properties. That being said, the difficulty of computing the inverse
Hessian in this particular case makes this option rather
unappealing. For this reason, we will use steepest descent.

It is important to note that, as a matter of practicality, this method
will require an initial feasible solution. For this reason, we will
begin the iteration with a weak $\chi^2$ tolerance $c$. This can then
be tightened as the iteration progresses.

### A naive approach

In addition to the formulation describe above, one can also envision
an optimization program which puts $\chi^2$ and the entropy $S$ on an
equal footing. In this case, one could construct an objective with a
weight parameter $\beta$ to adjust the balance between these two
terms,

\begin{equation*}
\max_\mathbf{a} (1-\beta) S(\mathbf{a}) - \beta \chi^2
\end{equation*}

## Test data generation

For our experiments we sampled a number of diffusivities from a two
component log-normal mixture. We then computed values of the
correlation function induced by these species at a number of
logarithmically spaced time $\tau$s. To reproduce the imperfection of
experimental data, small amplitude Gaussian noise was added to th

# Results

## Projected subgradient method

This approach gave a great deal of difficulty. Unfortunately,
convergence was extremely sensitive to the parameters. Frequently the
projection would enter loops wherein the weights would all reach zero.

The iteration was generally run with a constant step size as this
provided the most reliable convergence. Unfortunately, tuning this
parameter was extremely unforgiving as small values would result in
oscillations while large values would simply not converge.

Given the mode of failure, it seems that the crude projection operator
used here is to blame. A more justified approach minimizing the
projected distance under a constraint holding the projection on the
surface of the quadric might be more effective. That being said, given
that projected subgradient is best used when the projection is easy to
compute (a condition which even the line search doesn't satisfy), this
is likely a sign that this approach is simply not suited for this
problem.

## Logarithmic barrier method




\appendix

# Bibliography
